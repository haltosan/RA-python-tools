{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs (in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of what this notebook does:\n",
    " - Reads in text from files\n",
    " - Splits text into sections based on a user-specified regex (for example, separates output from different images that are in the same OCR file)\n",
    " - Runs spaCy on each section, which identifies names, dates, and locations in the words (not super well, but decently)\n",
    " - Uses regex to filter through the names and dates and pick out the ones most likely to be relevant (currently, it chooses the first full name and date in the hopes it's the name and death date of the person in the obituary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import spacy # using for NER (Named Entity Recognition)\n",
    "\n",
    "# start debugging for encoding errors\n",
    "import locale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions/Other Preliminary Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the lines in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(fname):\n",
    "    enc = locale.getpreferredencoding()\n",
    "    with open(fname, encoding=enc) as fin:\n",
    "        lines = fin.read()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text by the image it came from\n",
    "\n",
    "TODO: define the regex you want to use to split the file into different entries, and turn on/off the option to delete the last string in the list (depending on the format of the input, may always be empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input(text):\n",
    "    \"\"\"Splits text into different strings, using a regex as a seperator\"\"\"  \n",
    "    if type(text) is not str:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PUT TEXT-SPLITTING REGEX HERE\n",
    "    split_re = r'--- PAGE END ---'\n",
    "    split_strings = re.split(split_re, text)\n",
    "    \n",
    "    # IF EACH SECTION WILL HAVE A MATCH FOR SPLIT_RE AT THE END, RUN THIS TO DELETE THE LAST STRING IN THE LIST (WILL BE EMPTY)\n",
    "    last_is_empty = True\n",
    "    if last_is_empty and len(split_strings) > 0:\n",
    "        del split_strings[-1] # removes empty string at the end\n",
    "    return split_strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give file information\n",
    "\n",
    "TODO: name the output file and give paths for the input and output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GIVE A NAME FOR THE OUTPUT FILE HERE (not a path, just something recognizable)\n",
    "output_name = \"weekend_end\"\n",
    "\n",
    "# Name of the csv file to write to\n",
    "target = f\"{output_name}.csv\"\n",
    "\n",
    "# PUT INPUT FOLDER HERE\n",
    "INPUT_PATH = r'V:\\papers\\current\\tree_growth\\US\\Skagit\\skagit_obits\\2_NLP\\input'\n",
    "\n",
    "# PUT OUTPUT FOLDER HERE\n",
    "OUTPUT_PATH = r'V:\\papers\\current\\tree_growth\\US\\Skagit\\skagit_obits\\2_NLP\\output'\n",
    "\n",
    "os.chdir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Makes the output replace the previous file, instead of appending to the previous file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update target file name so that we aren't appending to an existing file\n",
    "    \n",
    "if os.path.exists(target):\n",
    "    i = 1\n",
    "    name, ext = target.split('.')\n",
    "    while os.path.exists(f'{name}_{i}.{ext}'):\n",
    "        i += 1\n",
    "    target = f'{name}_{i}.{ext}'\n",
    "\n",
    "# (took out code for the check file; don't know how to create one with this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Regex\n",
    "Go to https://regex101.com/ to test your regex. Currently, this notebook needs a regular expression for death date, name, and image name.\n",
    "\n",
    "TODO: put any regex you'll be using to filter data or get image names here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT DEATH DATE REGEX HERE\n",
    "date_re = re.compile(r'(?P<month>Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)\\.? (?P<day>\\d{1,2})(?:, (?P<year>\\d{4}))')\n",
    "# PUT NAME REGEX HERE\n",
    "name_re = re.compile(r'(?P<firstNames>(?:[\\'\\\"]?[A-Z][\\'\\\"A-Za-z]+ )+(?:[A-Z]. )*)(?P<lastName>[A-Z][\\'A-Za-z]+)')\n",
    "# PUT IMAGE NAME REGEX HERE (if you're outputting the image name into the OCR)\n",
    "image_re = re.compile(r'IMAGE:(?P<imageName>[^\\n]+)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define column names (will be printed at top of file)\n",
    "\n",
    "TODO: If you want column names printed at the top, write the titles you want in order here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Image','First_Names', 'Last_Name', 'Death_Date_Day', 'Death_Date_Month', 'Death_Date_Year', 'Names', 'Dates','Locations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spacy(text):\n",
    "    \n",
    "    # lists that hold the named entities in the obituary\n",
    "    names = []\n",
    "    dates = []\n",
    "    locations = []\n",
    "    \n",
    "    matches = NER(text)\n",
    "    \n",
    "    date_count = 0\n",
    "    gpe_count = 0\n",
    "    \n",
    "    for word in matches.ents:\n",
    "        if word.label_ == 'PERSON':\n",
    "            names.append(word.text)\n",
    "        elif word.label_ == 'DATE':\n",
    "            dates.append(word.text)\n",
    "            date_count += 1\n",
    "        elif word.label_ == 'GPE':\n",
    "            locations.append(word.text)\n",
    "            gpe_count += 1\n",
    "            \n",
    "    return names, dates, locations    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "\n",
    "These run through either the name, date, or location list and return the first list entry that matches the regex (formatted). If necessary, add more functions with the variations you need to get specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death date\n",
    "\n",
    "Returns the first full date in the obituary, which is hopefully the death date\n",
    "\n",
    "Note: uses capturing groups named 'month', 'day', and 'year'. Change if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def death_date(dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        re_match = date_re.search(date)\n",
    "        if re_match:\n",
    "            month = month_to_int(re_match.group('month')) # convert text month to the month number\n",
    "            day = int(re_match.group('day'))\n",
    "            year = int(re_match.group('year'))\n",
    "            return [day, month, year]\n",
    "        \n",
    "    return [0, 0, 0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus Person's Name\n",
    "\n",
    "Returns the first full name in the obituary (which is hopefully a name and hopefully the person the obituary is about)\n",
    "\n",
    "Note: uses capturing groups called 'firstNames' and 'lastName'. Change if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_person(names):\n",
    "    \n",
    "    for name in names:\n",
    "        re_match = name_re.search(name)\n",
    "        if re_match:\n",
    "            first_names = re_match.group('firstNames')\n",
    "            last_name = re_match.group('lastName') \n",
    "            return [first_names, last_name] \n",
    "    \n",
    "    return ['', '']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image name\n",
    "\n",
    "Get the image name out of the OCR\n",
    "\n",
    "Note: uses a named capturing group called 'imageName'. Change if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_name(string):\n",
    "    image = image_re.search(string)\n",
    "    if image:\n",
    "        return image.group('imageName')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print labels\n",
    "\n",
    "Prints out each named entity that it found along with the label it gave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labels(matches): # param: output from NER\n",
    "    print(text)\n",
    "    print('')\n",
    "    for word in matches.ents:\n",
    "        print(word.text, word.label_)\n",
    "    print('#############################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert month to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_int(month):\n",
    "    if month == 'Jan' or month == 'January':\n",
    "        return 1\n",
    "    elif month == 'Feb' or month == 'February':\n",
    "        return 2\n",
    "    elif month == 'Mar' or month == 'March':\n",
    "        return 3\n",
    "    elif month == 'Apr' or month == 'April':\n",
    "        return 4\n",
    "    elif month == 'May':\n",
    "        return 5\n",
    "    elif month == 'Jun' or month == 'June':\n",
    "        return 6\n",
    "    elif month == 'Jul' or month == 'July':\n",
    "        return 7\n",
    "    elif month == 'Aug' or month == 'August':\n",
    "        return 8\n",
    "    elif month == 'Sep' or month == 'Sept' or month == 'September':\n",
    "        return 9\n",
    "    elif month == 'Oct' or month == 'October':\n",
    "        return 10\n",
    "    elif month == 'Nov' or month == 'November':\n",
    "        return 11\n",
    "    else:\n",
    "        return 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code Body\n",
    "\n",
    "Does the active stuff. If you add more functions to filter out additional information, etc., put code here to call that function and output the results in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding obituaries in 10.txt...\n",
      "found 200 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 11.txt...\n",
      "found 285 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 12.txt...\n",
      "found 180 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 13.txt...\n",
      "found 215 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 14.txt...\n",
      "found 230 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 15.txt...\n",
      "found 200 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 16.txt...\n",
      "found 300 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 17.txt...\n",
      "found 240 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 18.txt...\n",
      "found 120 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 19.txt...\n",
      "found 350 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 2.txt...\n",
      "found 105 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 20.txt...\n",
      "found 545 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 21.txt...\n",
      "found 220 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 22.txt...\n",
      "found 195 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 23.txt...\n",
      "found 305 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 24.txt...\n",
      "found 255 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 25.txt...\n",
      "found 175 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 26.txt...\n",
      "found 375 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 27.txt...\n",
      "found 355 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 28.txt...\n",
      "found 310 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 29.txt...\n",
      "found 415 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 3.txt...\n",
      "found 340 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 30.txt...\n",
      "found 305 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 31.txt...\n",
      "found 165 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 32.txt...\n",
      "found 295 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 33.txt...\n",
      "found 185 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 34.txt...\n",
      "found 195 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 35.txt...\n",
      "found 135 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 36.txt...\n",
      "found 70 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 37.txt...\n",
      "found 180 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 38.txt...\n",
      "found 115 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 39.txt...\n",
      "found 175 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 4.txt...\n",
      "found 200 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 40.txt...\n",
      "found 120 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 5.txt...\n",
      "found 115 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 6.txt...\n",
      "found 280 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 8.txt...\n",
      "found 60 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in 9.txt...\n",
      "found 335 obituaries. Writing to file spacy_test_4.csv...\n",
      "finding obituaries in z-1.txt...\n",
      "found 355 obituaries. Writing to file spacy_test_4.csv...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(OUTPUT_PATH)\n",
    "\n",
    "# spacy stuff - disable makes sure it loads only the NER part\n",
    "NER = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "with open(target, 'a', newline='', encoding='utf-8-sig') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(column_names)\n",
    "    \n",
    "    os.chdir(INPUT_PATH)\n",
    "    \n",
    "    # runs through each file in the input folder\n",
    "    # CHANGE STUFF HERE TO MAKE IT GO IN ORDER\n",
    "    for txt in [i for i in os.listdir() if i[-4:] == '.txt']:\n",
    "        os.chdir(INPUT_PATH)\n",
    "    \n",
    "        # Use the split_input() function to split the text document, so the OCR output for each source image is separate\n",
    "        print(f'finding obituaries in {txt}...')\n",
    "        split_strings = split_input(pre_process(txt))\n",
    "        num = len(split_strings)\n",
    "        print(f'found {num} obituaries. Writing to file {target}...')\n",
    "    \n",
    "        os.chdir(OUTPUT_PATH)\n",
    "        \n",
    "        # Run spaCy on the OCR text from each image and filter it\n",
    "        for obit in split_strings:\n",
    "            \n",
    "            string = obit.replace('\\n', ' ') # take out newlines\n",
    "            string = string.replace('- ', '') # take out gaps where it was one word\n",
    "            \n",
    "            names = []\n",
    "            dates = []\n",
    "            locations = []\n",
    "            \n",
    "            columns = [] # list of strings that will become each entry in a row\n",
    "            \n",
    "            # get image name\n",
    "            image_name = get_image_name(string)\n",
    "            columns.append(image_name)\n",
    "                \n",
    "            # use NER to get lists of names and locations\n",
    "            names, dates, locations = run_spacy(string)\n",
    "            \n",
    "            # use regex to find the focus person (first full name in the NER stuff)\n",
    "            name_sections = focus_person(names)\n",
    "            columns.extend(name_sections)\n",
    "            \n",
    "            # use regex to find death date (first full date in the NER stuff)\n",
    "            date_sections = death_date(dates)\n",
    "            columns.extend(date_sections)\n",
    "            \n",
    "            # IF YOU USE REGEX TO FILTER MORE DATA, ADD IT TO THE COLUMNS HERE\n",
    "            \n",
    "            # combine names, dates, and locations into one string per category and output\n",
    "            names_str = \";\".join(names) # makes a list into a single string\n",
    "            dates_str = \";\".join(dates)\n",
    "            locations_str = \";\".join(locations)\n",
    "            columns.extend([names_str, dates_str, locations_str])\n",
    "            \n",
    "            # Write out the row\n",
    "            writer.writerow(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
